Set-Alias -Name k -Value kubectl

//get entire environment
k get all


run
kubectl proxy

To get a token in another window

kubectl -n kubernetes-dashboard create token admin-user


open
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login



after enabling you can set an alias
//only sets if for a session


Set-Alias -Name k -Value kubectl

PS C:\Users\jfabi> k version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2


PS C:\Users\jfabi> k cluster-info
Kubernetes control plane is running at https://kubernetes.docker.internal:6443
CoreDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

***********Web UI Dashboard ***********

Step to use:
Install the Kubernetes Dashboard:

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
Run

kubectl apply -f dashboard.adminuser.yml
Get a token (see https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md) by running the command shown below. Copy the token into your clipboard.

kubectl -n kubernetes-dashboard create token admin-user
Run

kubectl proxy
Visit http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

Paste the token into the login screen and you can then sign in to the dashboard.

***********POD Concepts
Smallest Object of the kub object model
env for containers
organization applicaion parts into pods
(server, caching, APIs, database, etc)

Normally you have a single process per container and single container per pod

Pod has IP, memory, volumes, etc shared accross containers
Scale horizontally by adding Pod replicas

Pods live and die but never come back to life


Pods containers shared the same network namespaces (shareip/port)
Pod containers have the same loopback network interface (localhost)

Container process need to bind to different ports within a pod

Pods can't span nodes

***********Create Pods via kubectl or yaml

Podas and containers are only accessible within a Kube cluster by default

One way to expose a container port externally:
 kubectl port-forward


#Enable Prod container to be called externally - Most basic way
kubectl port-forward [name-of-pod] 8080:80

#Will cause pod to be recreated - keeps the deployment healthy be recreating the pod
kubectl delete pod [name-of-pod]

#Delete Deployment if you dont want the pod to be recreated then delete the deployment
kubectl delete deployment [name-of-deployment]


***********Kubectl and pods
Set-Alias -Name k -Value kubectl
k run my-nginx --image=nginx:alpine
k get pods
k port-forward my-nginx 8080:80
k delete pod my-nginx

***********Via Yaml
#Basic Pod yml

apiVersion: v1
kind: Pod
metadata:
  name: my-nginx
spec:
  containers:
  - name: my-nginx
    image: nginx:alphine



k create -f nginx.pod.yml --dry-run --validate=true

#Alternate way to create or apply changes to a pod, it will create if not existing or will update if existing
k apply -f file.pod.yml

# Use --save-config when you want to use kubectl apply in the future
k apply -f file.pod.yml --save-config

# Delete Pod using yaml file
k delete -f file.pod.yml


k describe pod my-nginx



******Access to Inside the Container
//exec into container, use -it for interactive and sh for shell

k exec -it my-nginx -- sh  //new way of exec into container


*****Pod Health / Probes
Liveness
Readiness


**********Creating Deployments /ReplicaSet
A ReplicaSet is a declarative way to manage Pods
A Deployment is a declarative way to manage Pods using a ReplicaSet

Deployment and ReplicaSets ensure Pods stay running and be be used to scale Pods

ReplicaSets act as a Pod controller:

- Self-healing mechanism
- Ensure the requested number of  Pods are available
- Provide Fault-TOlerance
- Can be used to scale Pods
- No need to create Pods directly
- Used by deployments


A Deployment manages Pods:

- Pods are managed using ReplicaSets
- Scales ReplicaSets, which scale Pods
- Supports zero-downtime updates by creating and destroying ReplicaSets
- Provides rollback Functionality
- Create a unique label that is assigned to the ReplicaSet and generated Pods
- Yaml is very similar to a ReplicaSet 



**********Creating Deployment Using Yaml
Dont need to create the repolicasets

(just use apply since it will create if doesn't exists or updates, no need to use create)

k apply -f nginx.deployment.yml 

//get deploymensts
k get deploy
k get deployments

//get deployments with labels
k get deployments --show-labels

//get deployments with specific labels -l

k get deployment -l app=nginx

k delete deployment [deployment-name]

//delete via yaml
k delete deployment -f  nginx.deployment.yml 

//scale deployment
k scale deployment [deployment-name] --replicas=5

You can set the replicates in the YML Files as well

spec:
  replicas: 3



Resource Constraits - allows to constrain a container to certain resources which is very important, need constraints so kubernetes isn't a runaway train
spec:
      containers:
      - name: my-nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        resources:
          limits:
            memory: "128Mi" #128 MB
            cpu: "200m" #200 millicpu (.2 cpu or 20% of the cpu)






k get deploy -l app=my-nginx

k create -f nginx.deployment.yml --save-config


//create 4 replicas
k scale -f nginx.deployment.yml --replicas=4






**********Z Downtime Deployment Options

How do You update existing pods?

Current Pod               Desired Pod
nginx:1.14.2-alpine ----> nginx1.15.9-alpine


Zero downtime dployments allow software updates to be deployed to production without impacting end users which if one the strengths

update an applicatin's pods without impacting end users

Several Options are available:
- Rolling updates - rolling is updating pod then deploying one by one
- Blue-green deployments (multiple environments running exactly at same time)
- Canary deployments small amount of traffic goes to new deployment until 
- Rollbacks - roll back to previous version



Once the new pod is ready one of the older pods can be deleted, repeat...etc 


//update a deployment by changing the yaml and applying changes to the cluster with kubectl apply
k apply -f file.deployment.yml


***Deployments in action
k apply -f node-app-v1.deployment.yml
k apply -f node-app-v2.deployment.yml
k apply -f node-app-v3.deployment.yml


Set-Alias -Name k -Value kubectl


**********Services
A  Service provides a single point of entry for accessing one or more Pods

Since Pods live and die, you can't rely on the ip address staying the same

A Pod gets an IP Address after it has been scheduled (no way for clients to know iP Ahead of time)


Services abstract POD ip addresses from consumers


my-app (Service) has a fixed ip but it knows how to talk to the pods behind it (labels are important since can associate pods with a service via the label)

It will know how to Load balances between pod which is a built in feature

Node's kube-proxy creates a virutla ip for the services

Layer 4 (TCP/UDP or IP)

Service are not ephemeral, they need to stick around

Creates endpoints which sits between a Service and POD


service 10.0.0.1:80 (label frontend ) ----> Pod 10.0.0.43:8080 (label frontend) --->  service (label backend) 10.2.0.1:9000  --->  Pod (label backend) 10.2.0.10:27017  
                                     ---->  Pod 10.0.0.53:8080 (label frontend) ---> 


**Service Types                                     
4 main types

1 - ClusterIP - Expose the service on a cluster-internal IP (Default)
2 - NodePort - Expose the service on each Node's IP at a static port
3 - LoadBalancer - Provision an external ip to act as a load balancer for the service
4 - ExernalName - Maps a service to a DNS name


**ClusterIP 
Service PI is exposed internallly within the cluster
Only Pods within a cluster can talk to the service
Allows Pods to talk to other Pods

**NodePort
Exposes the Service on each Node's IP at a static port
Allocates a port from a range (default is 30000 - 32767)
Each Node proxies the allocated port


Node (port 30100) - > Service (nodeport) ---> Pod1
                                         ---> Pod2

**LoadBalancer
routes to different nodes based on traffic
exposes a service externally
useful when combined with a cloud provider's load balancer
There is an nginx one and a few others


external caller --> LoadBalancer(x.x.x.x:80) ------> Node(30105) Service (nodeport)
                                             ------> Node(30105) Service (nodeport)
**ExernalName 
Service that acts as an alias for an external service
Allows a service to act as a proxy for an external service
External servcice details are hidden from cluster (easeir to change) ips might keep changing 
Ability to call to external service that a pod might call



*****Creating a servcie with kubectl
Q: How can you access a pod from outside of Kubernetes (you can't)
A: Port Forwarding
use kubctl port-forward to forard to a local port to a Pod Port

//for a pod
k port-forward pod/[pod-name] 8080:80  

//for a deployment
k port-forward deployment/[deployment-name] 8080

k port-forward deployment/my-nginx 8080:80 (needs to forward the internal to 80 since that is what nginx runs on)

//for a service
k port-forward service/[service-name] 8080



*****Storage Options

How do you store application state/data and exchange it betweek Pods and Kubernetes

Answer: Volumes (although other data storage options exist)

Podss live and die so their file system is short lived (ephemeral)

Volumes can be used to store state/data and use it in a Pod

A Pod can have multiple Volumes attached to it

Containers rely on a mountPath to access a Volumne

Kubernetes Supports:
- Volumes
- PersistentVolumes
- PersistentVolumeClaims
- StorageClasses

*****Storage Options - Volumes
A volume references a storage location
Must have a uniqe name
Attached to a Pod and may or may not be tied to the Pod's lifetime (depending on the volumne type)
A Volumne Mount references a Volume by name and defines a mountPath


**Volume Types: (NOTE: There are a lot of volumne types. cloud and 3rd party)
 emptyDir - empty directory for storying "transient" data (shares a Pod's lifetime), useful for shraing files betwewen containers running in a pod 
 hostPath - Pod mounts into a node's filesystem
 nfs - An NFS (Network File system) shared mounted into a pod
 configMap/secret = Special types of volumes that provie a Pod with access to Kubernetes resouorces
 persistentVolumeClaim - provides pods with a more persistent storage option that is abstracted from the details
 cloud - cluster-wide storage

**PersistentVolume and PersistenceVolumeClaim

A persistent Volume (PV) is a cluster wide storage unit provisioned by an Administrator with a lifecycel independent from a Pod
relies on some type of network attached storage (NAS)

Normally provisioned by a cluster admin
Available to a POD even if it gets rescheduled to a different Node
Rely on storage provider such as NFS, cloud storage and other options
Associated with a POD by using a PersistentVolumeClaim (PVC)



Pod or other resource -->  PVC ---> PV | ---> storage
                                       | ---> Kubernetes


//great site to get examples
https://github.com/kubernetes/examples 



*****Storage StorageClasses

A StorgeCLass (SC) is a type of storage template that can be used to dynamically provision storage

Used to define different "classes" of storage
Acts as a type of storage template
Supports dynaimc provisioning of the PersistentVolumes (makes it much more flexible)
The others were static, one by one

1 - Create a Storage Class via yaml
2 - Create PersistentVolumeClaim that references the StorageClass
3 - Kubernetes uses the StorageCLass provisioner to provision a PersistentVolume

POD ---> PVC ---> SC ---> PV Provisioner ---> PV ---> Storage



*****ConfigMaps and Secrets
ConfigMaps provide a way to store configration information and provide it to containers
Provides a way to inject configuration data into a container
Can store entire files or provide key/value pairs

- Store in a File, Key is the filename, value is the file contents (can be JSON, XML, key/values, etc)

Pod ----> ConfigMap

 Proivde on the command-Line 
 ConfigMap Manifest (yaml file)

 Pod - acess configMaps eith by
 - environment variables
 - configMap volume


 ****Creating a ConfigMap

 ConfigMap Manifest yaml 

 //Create a ConfigMap using data from a file, the file name becomes the key 
 k create configMap [cm-name] --from-file=[path-to-file]


**Via env file - will look like the manifest, no file name as key, value is the entire file
k create configMap [cm-name] --from-env-file=[path-to-file]

 ****Using a ConfigMap

//Get a ConfigMap and see the yaml
 k get cm [cm-name] -o yaml  

//get all
 k get cm


********Code With Dan Services sample app---See sample app
//instead of each file separately with -f you can apply to the entire directory
k apply -f .k8s 


***Debugging

//view logs for a pod's container
k logs [pod-name]

//view logs for a specific container within a pod
k logs [pod-name] -c [container-name]

//view logs for a previously running pod
k logs -p [pod-name]

// Stream a pod's logs
k logs -f [pod-name]



